{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b20425",
   "metadata": {},
   "source": [
    "# Why Convex optimization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57259a",
   "metadata": {},
   "source": [
    "Convex optimization, as a powerful mathematical tool, has demonstrated its importance across various fields, particularly in machine learning and big data analysis. Future developments in convex optimization may revolve around the following trends and research directions:\n",
    "\n",
    "Deeper Integration with Machine Learning: Many problems within the fields of machine learning, especially deep learning, can be addressed through optimization methods. Future research might explore how to utilize the theory and methods of convex optimization to improve existing machine learning algorithms, such as through more effective regularization techniques or new loss functions, to ensure the convergence and robustness of the algorithms.\n",
    "\n",
    "Convex Approximation of Non-convex Problems: Although convex optimization offers good theoretical guarantees and efficient solving methods, many practical problems are inherently non-convex. Future studies may focus on how to construct effective convex approximations or develop new convexification techniques, to keep the problems solvable while closely approximating the original problems.\n",
    "\n",
    "Optimization for Big Data: With the continuous growth of data volumes, efficiently handling large-scale optimization problems becomes a challenge. Future research will explore how to utilize distributed computing, online learning, and stochastic optimization techniques to accelerate convex optimization algorithms, enabling them to effectively handle complex problems in big data environments.\n",
    "\n",
    "Combining Theory and Practice: While convex optimization is well-supported theoretically, further exploration is needed on how to adjust and optimize algorithms to achieve the best performance in specific application scenarios. Future research may focus on how to merge theoretical research with practical applications, developing more practical and efficient customized optimization solutions.\n",
    "\n",
    "Interdisciplinary Applications: Convex optimization techniques have already shown extensive application potential in fields such as finance, bioinformatics, and energy management. More interdisciplinary collaborations are expected in the future, exploring the applications of convex optimization in additional areas like intelligent transportation systems, environmental engineering, and public health.\n",
    "\n",
    "Adaptive and Intelligent Optimization Algorithms: With the advancement of artificial intelligence technologies, making optimization algorithms more adaptive and intelligent‚Äîcapable of automatically selecting or adjusting algorithm parameters based on problem characteristics‚Äîis a likely direction for future research.\n",
    "\n",
    "Through these research directions, convex optimization not only provides a robust tool for addressing traditional mathematical and engineering problems but could also play a significant role in cutting-edge domains such as data science and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbbe94",
   "metadata": {},
   "source": [
    "# What is convex optimization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97649d7",
   "metadata": {},
   "source": [
    "Understanding convex optimization geometrically can indeed be facilitated by visualizing shapes like parabolas. Typically, we use parabolas to describe quadratic functions, which are very intuitive in one-dimensional (one variable) or two-dimensional (two variables) contexts.\n",
    "\n",
    "#### One-Dimensional Convex Function: \n",
    " Parabola Consider the most common one-dimensional convex function, a quadratic function ùëì ( ùë• ) = ùëé ùë• 2 + ùëè ùë• + ùëê f(x)=ax 2 +bx+c, where ùëé > 0 a>0. The graph of this function forms an upward-opening parabola. In this case, the lowest point of the parabola (the vertex) represents the global minimum of the function.\n",
    "\n",
    "#### Two-Dimensional Convex Function: Paraboloid\n",
    "\n",
    " Extending the concept of a convex function to two dimensions, consider a function ùëì ( ùë• , ùë¶ ) = ùëé ùë• 2 + ùëè ùë¶ 2 + ùëê f(x,y)=ax 2 +by 2 +c, where ùëé > 0 a>0 and ùëè > 0 b>0. The graph of this function forms a paraboloid. This paraboloid is convex in any direction viewed, meaning every cross-section, whether horizontal or vertical, is convex. In this scenario, the lowest point (vertex) of the paraboloid represents the function‚Äôs global minimum.\n",
    "\n",
    "#### Geometric Interpretation of Convex Sets\n",
    "\n",
    "For a geometric interpretation of convex sets, you can visualize a shape where any line segment connecting two points lies entirely within the figure. For example, in two-dimensional space, both a circle and a rectangle are convex sets because any line segment connecting two points within these shapes remains entirely inside the shapes.\n",
    "\n",
    "#### Geometric Characteristics of Convex Optimization Problems\n",
    "\n",
    "In convex optimization problems, we seek the point within a defined range of a convex set that minimizes the value of a convex function. Geometrically, this means we are looking for the lowest point on a convex shape (like a paraboloid), while this point also satisfies any existing constraints (such as needing to be within a specific convex set region).Through this approach, convex optimization leverages the properties of convex functions and convex sets to ensure global optimality of the solution, as well as the efficiency and stability of the algorithm. This geometric intuitiveness makes convex optimization a powerful tool for solving practical problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a689cc4",
   "metadata": {},
   "source": [
    "# The three most common algorithms in convex optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f0e3d",
   "metadata": {},
   "source": [
    "In convex optimization, the three most common and widely used algorithms are Gradient Descent, Interior-Point Methods, and Newton's Method. Each of these algorithms has unique characteristics and advantages, making them suitable for different types of optimization problems. Here are detailed descriptions of these three methods:\n",
    "\n",
    "## 1 Gradient Descent (Gradient Descent) \n",
    "Gradient Descent is one of the most direct and commonly used methods for solving optimization problems. It is suitable for large-scale problems and is simple to implement. \n",
    "##### Principle: \n",
    "In each step, Gradient Descent updates the variable ùë• x to descend along the gradient of the function (i.e., the opposite direction of the steepest ascent). \n",
    "##### Update Rule:\n",
    "X(k+1)=X(k) ‚àíŒ±‚àáf(X(k)), where ùõº Œ± is the learning rate, ‚àá ùëì (X(k))  is the gradient at point X(k)\n",
    "##### Advantages: \n",
    "Simple implementation, easy to understand. Disadvantages: The convergence speed may be slow, especially when approaching the minimum value point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc224eb0",
   "metadata": {},
   "source": [
    "##### Practical Example\n",
    "Here's an example using Python to demonstrate the Gradient Descent algorithm for solving a simple quadratic optimization problem. In this example, we aim to find the minimum of the function ùëì ( ùë• ) = ùë• 2 + 10 ùë• + 25. This function is convex, and its global minimum can be easily calculated, with the derivative being ùëì '( ùë• ) = 2 ùë• + 10 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cb31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = -1.0, f(x) = 16.0\n",
      "Iteration 2: x = -1.8, f(x) = 10.24\n",
      "Iteration 3: x = -2.4400000000000004, f(x) = 6.553599999999996\n",
      "Iteration 4: x = -2.9520000000000004, f(x) = 4.194303999999999\n",
      "Iteration 5: x = -3.3616, f(x) = 2.6843545600000027\n",
      "Iteration 6: x = -3.68928, f(x) = 1.7179869184000012\n",
      "Iteration 7: x = -3.9514240000000003, f(x) = 1.0995116277760033\n",
      "Iteration 8: x = -4.1611392, f(x) = 0.703687441776637\n",
      "Iteration 9: x = -4.32891136, f(x) = 0.4503599627370498\n",
      "Iteration 10: x = -4.4631290880000005, f(x) = 0.2882303761517093\n",
      "Final x value: -4.4631290880000005\n",
      "Final function value: f(-4.4631290880000005) = 0.2882303761517093\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"Define the objective function.\"\"\"\n",
    "    return x**2 + 10*x + 25\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative of the objective function.\"\"\"\n",
    "    return 2*x + 10\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    \"\"\"Implementation of the gradient descent algorithm.\"\"\"\n",
    "    x = initial_x\n",
    "    for i in range(num_iterations):\n",
    "        grad = df(x)  # Calculate the gradient at the current position\n",
    "        x = x - learning_rate * grad  # Update the value of x\n",
    "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}\")\n",
    "    return x\n",
    "\n",
    "# Parameter settings\n",
    "initial_x = 0  # Starting point\n",
    "learning_rate = 0.1  # Learning rate\n",
    "num_iterations = 10  # Number of iterations\n",
    "\n",
    "# Execute the gradient descent\n",
    "final_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
    "print(f\"Final x value: {final_x}\")\n",
    "print(f\"Final function value: f({final_x}) = {f(final_x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c6576",
   "metadata": {},
   "source": [
    "#### Code Explanation: \n",
    "The f(x) function defines the function we want to minimize. The df(x) function calculates the derivative of f(x), used to compute the gradient. The gradient_descent function performs the gradient descent algorithm where initial_x is the initial point, learning_rate controls the step size per update, and num_iterations is the count of iterations. During each iteration, we print the current value of x and the function value f(x) to monitor the progress of the algorithm.\n",
    "\n",
    "#### Output Explanation:\n",
    "The code outputs the current value of x and the corresponding f(x) after each iteration. This allows you to see how x gradually approaches the point of minimum and how the function value changes. Finally, we obtain an x value close to the minimum point and output the function value at that point.\n",
    "\n",
    "This simple example clearly demonstrates how the Gradient Descent method works and how it is used to find the minimum of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae73732",
   "metadata": {},
   "source": [
    "## 2 Interior-Point Methods\n",
    "\n",
    "Interior-Point Methods are an efficient algorithm for solving constrained optimization problems, particularly well-suited for handling linear programming and quadratic programming problems. This method was introduced by Karmarkar in the late 1980s and has since become a primary solution technique for linear programming and other types of optimization problems.\n",
    "\n",
    "#### Principle and Working Method\n",
    "The core idea of the Interior-Point Method is to transform a constrained optimization problem into a series of unconstrained problems, which progressively approximate the solution to the original constrained problem. This transformation typically involves incorporating a component called a barrier function (or barrier term) into the objective function. This barrier function becomes significantly large as the solution approaches the boundaries of the feasible domain, thereby implicitly keeping the solution within these boundaries.\n",
    "\n",
    "#### Barrier Function:\n",
    "The most common type of barrier function is the logarithmic barrier function. For each inequality constraint ùëî ùëñ ( ùë• ) ‚â§ 0 , the barrier function could be ‚àí ‚àë log ‚Å° ( ‚àí ùëî ùëñ ( ùë• ) ) As ùë• x approaches the boundary of the constraint, i.e., as ùëî ùëñ ( ùë• )  approaches 0, ‚àí log ‚Å° ( ‚àí ùëî ùëñ ( ùë• ) )tends toward positive infinity, thus numerically \"preventing\" ùë• from crossing the boundary.\n",
    "\n",
    "### Linear Programming Example: \n",
    "###### Objective: \n",
    "Minimize ùëì ( ùë• ) = 3 ùë• 1 + 4 ùë• 2  \n",
    "###### Constraints: \n",
    "ùë• 1 + 2 ùë• 2 ‚â• 8  \n",
    "5 ùë• 1 + 2 ùë• 2 ‚â§ 10  \n",
    "ùë• 1 ‚â• 0 , ùë• 2 ‚â• 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2fe41",
   "metadata": {},
   "source": [
    "#### Transformation and Setup\n",
    "First, we'll transform the inequalities into a format suitable for the Interior-Point Method. For the constraint ùë• 1 + 2 ùë• 2 ‚â• 8 , we introduce a slack variable ùë† 1 and convert it to the equation ùë• 1 + 2 ùë• 2 ‚àí ùë† 1 = 8 . For 5 ùë• 1 + 2 ùë• 2 ‚â§ 10 , we add another slack variable ùë† 2  and convert it to 5 ùë• 1 + 2 ùë• 2 + ùë† 2 = 10 .\n",
    "#### Application of Interior-Point Method\n",
    "#### 1 Initialization:\n",
    "Choose an initial point, such as ùë• 1 = 1 , ùë• 2 = 3 , ùë† 1 = 1 , ùë† 2 = 1 , ensuring all variables are positive, which complies with the requirements of the Interior-Point Method.\n",
    "\n",
    "#### 2 Barrier Function:\n",
    "Introduce a barrier function Œ¶ ( ùë• ) = ‚àí ùúá ( log ‚Å° ùë† 1 + log ‚Å° ùë† 2 ) , where ùúá  is a large positive number, like ùúá = 1.\n",
    "\n",
    "#### 3 Iterative Solution:\n",
    "Use Newton's method to solve the unconstrained optimization problem: minimize ùëì ( ùë• ) + Œ¶ ( ùë• ) . \n",
    "Compute and update the values of ùë• 1 , ùë• 2 , ùë† 1 , ùë† 2  .\n",
    "\n",
    "#### 4 Update the Barrier Parameter:\n",
    "After each iteration, reduce the value of ùúá , e.g., multiply ùúá  by 0.1.\n",
    "\n",
    "#### 5 Termination Condition:\n",
    "Stop the iterations when ùúá Œº is sufficiently small, and the changes in the solution between consecutive iterations are minimal.\n",
    "\n",
    "#### Result Analysis\n",
    "As the iterations progress, the solution gradually approaches the true optimal solution while satisfying all the original constraints. By progressively reducing the impact of the barrier function (i.e., decreasing \n",
    "ùúá\n",
    "Œº), the solution can gradually move closer to the boundary of the constraints and finally stabilize at the optimal solution.\n",
    "\n",
    "This example clearly demonstrates how the Interior-Point Method converts a constrained problem into a series of progressively approximating unconstrained problems, thereby finding an optimal solution that meets all constraints. This method is particularly effective when dealing with large-scale problems involving many variables and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c11669",
   "metadata": {},
   "source": [
    "### Practical Example\n",
    " let's explore a  optimization problem using the Interior-Point Method in Python. We will solve a constrained nonlinear optimization problem using the scipy.optimize.minimize function.\n",
    "#### Problem Definition\n",
    "Minimize a nonlinear function, for example, ùëì ( ùë• , ùë¶ ) = ( ùë• ‚àí 1 )^ 2 + ( ùë¶ ‚àí 2 )^ 2 , where ùë• and ùë¶  are the variables.\n",
    "\n",
    "#### Constraints:\n",
    "x 2 +y 2 ‚â§10 (a constraint within a circular region) \n",
    "ùë• 3 + ùë¶ ‚â• ‚àí 2  (a nonlinear constraint)\n",
    "\n",
    "We will use the minimize function and specify trust-constr as the solver, which supports handling nonlinear constraints.\n",
    "\n",
    "#### Python Code Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4917c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: `xtol` termination condition is satisfied.\n",
      "Optimal value: 16.68774292610388\n",
      "Optimal solution: [ 0.24958463 -2.01554725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luozh\\anaconda3\\envs\\Python3-10\\lib\\site-packages\\scipy\\optimize\\_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint, BFGS\n",
    "import numpy as np\n",
    "\n",
    "# Objective function\n",
    "def objective(X):\n",
    "    x, y = X\n",
    "    return (x - 1)**2 + (y - 2)**2\n",
    "\n",
    "# Constraints function\n",
    "def cons_f(X):\n",
    "    x, y = X\n",
    "    return [x**2 + y**2, x**3 + y]\n",
    "\n",
    "# Bounds for constraints\n",
    "def cons_bounds(X):\n",
    "    return [10, -2]  # The first constraint should be <= 10, the second should be >= -2\n",
    "\n",
    "# Create a nonlinear constraint object\n",
    "nlc = NonlinearConstraint(cons_f, [-np.inf, -np.inf], cons_bounds(None), jac='2-point', hess=BFGS())\n",
    "\n",
    "# Initial guess\n",
    "x0 = [1.5, 1.5]\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(objective, x0, method='trust-constr', constraints=[nlc])\n",
    "\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Status:\", result.message)\n",
    "if result.success:\n",
    "    print(\"Optimal value:\", result.fun)\n",
    "    print(\"Optimal solution:\", result.x)\n",
    "else:\n",
    "    print(\"No solution found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11e444",
   "metadata": {},
   "source": [
    "#### Output Explanation\n",
    "\n",
    "The minimize function returns an optimization result object, which contains information about whether the solution was found successfully, the optimal value, and the optimal solution vector. NonlinearConstraint is used to define the nonlinear constraints. [-np.inf, -np.inf] and cons_bounds(None) represent the lower and upper bounds for the constraints. result.fun gives the value of the objective function at the optimum. result.x provides the values of the variables that minimize the objective function while satisfying the constraints.\n",
    "\n",
    "This example demonstrates how to use SciPy's minimize function along with NonlinearConstraint to solve a nonlinear optimization problem with constraints. This method is effective for handling complex nonlinear optimization problems and finding the optimal solution that satisfies all constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6b9e7",
   "metadata": {},
   "source": [
    "## 3 Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6468cc2",
   "metadata": {},
   "source": [
    "Newton's Method is a classical optimization algorithm used to find local minima (or maxima) of a function. It leverages the first and second derivatives of the function to locate the extremum points. The core idea of Newton's Method is to use a quadratic approximation via Taylor expansion to estimate the optimal point of the function, thereby accelerating the convergence process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c5fc0",
   "metadata": {},
   "source": [
    "#### Basic Principles of Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711b77f",
   "metadata": {},
   "source": [
    "Newton's Method is based on the following mathematical principle: For a target function ùëì ( ùë• ) f(x), it can be approximated near the current estimate ùë• ùëò  using Taylor expansion: ùëì ( ùë• ) ‚âà ùëì ( ùë• ùëò ) + ùëì ‚Ä≤ ( ùë• ùëò ) ( ùë• ‚àí ùë• ùëò ) + 1 /2 ùëì‚Ä≤‚Ä≤( ùë• ùëò ) ( ùë• ‚àí ùë• ùëò )^2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006a8bd",
   "metadata": {},
   "source": [
    "To find the value of ùë• x that minimizes ùëì ( ùë• ) , we take the derivative of the above expression and set it to zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5939a2",
   "metadata": {},
   "source": [
    "f‚Ä≤(xk)+f‚Ä≤‚Ä≤(xk)(x‚àíxk)=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50a434",
   "metadata": {},
   "source": [
    "Solving for ùë• x gives: ùë• = ùë• ùëò ‚àí ùëì ‚Ä≤ ( ùë• ùëò ) ùëì ‚Ä≤ ‚Ä≤ ( ùë• ùëò ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5f531",
   "metadata": {},
   "source": [
    "This is the one-dimensional case. When generalized to multiple dimensions, the first derivative (gradient) becomes a vector and the second derivative (Hessian) becomes a matrix. Thus, the update formula in the multi-dimensional case is: ùë• (ùëò + 1) = ùë• ùëò ‚àí ùêª ^(-1) ( ùë• ùëò ) ‚àá ùëì ( ùë• ùëò ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38283e",
   "metadata": {},
   "source": [
    "where ùêª ( ùë• ùëò )  is the Hessian matrix at ùë• ùëò  and ‚àá ùëì ( ùë• ùëò )  is the gradient at ùë• ùëò ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeec254",
   "metadata": {},
   "source": [
    "#### Applications of Newton's Method\n",
    "\n",
    "Newton's Method is widely used in various scientific and engineering problems due to its fast convergence properties, including:\n",
    "\n",
    "Optimization: \n",
    "In tuning parameters of machine learning models, especially when the parameter space is small or medium-sized.\n",
    "\n",
    "Solving Equations: \n",
    "In solving nonlinear systems of equations in physics and engineering problems.\n",
    "\n",
    "Economic Models: \n",
    "Calculating optimal strategies or equilibrium points in economic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe250a9",
   "metadata": {},
   "source": [
    "####  Efficiency and Stability of Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3c8c8",
   "metadata": {},
   "source": [
    "##### Efficiency: \n",
    "Newton's Method typically converges faster than simple gradient descent because it utilizes second-order derivative information. However, this also introduces additional computational burden, especially in large-scale problems requiring the computation and storage of large Hessian matrices.\n",
    "##### Stability: \n",
    "A potential issue with Newton's Method is that the Hessian matrix must be positive definite to ensure that each iteration moves towards a minimum. If the Hessian matrix is not positive definite or is near singular, Newton's Method may fail or exhibit unstable behavior. In practical applications, modifications like step size control (line search) or using quasi-Newton methods to avoid direct computation of the Hessian matrix can enhance the method's stability and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d389352",
   "metadata": {},
   "source": [
    "#### Solving a Practical Problem with Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e19287",
   "metadata": {},
   "source": [
    "To solve a practical problem using Newton's Method, we first need to identify the optimization problem and ensure that it has differentiable first and second derivatives. Here, we will demonstrate how to use Newton's Method to solve a simple optimization problem. Assume our goal is to minimize a real-valued function with available first and second derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9ddff",
   "metadata": {},
   "source": [
    "###### Step 1: Define the Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b91b",
   "metadata": {},
   "source": [
    "Assume we need to minimize the function: ùëì ( ùë• ) = ùë• 3 ‚àí 3 ùë• 2 + 2 f(x)=x 3 ‚àí3x 2 +2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101714f0",
   "metadata": {},
   "source": [
    "###### Step 2: Compute Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c229a4",
   "metadata": {},
   "source": [
    "First, compute the first and second derivatives of the function ùëì ( ùë• ) \n",
    "\n",
    "First derivative (gradient): ùëì ‚Ä≤ ( ùë• ) = 3 ùë• 2 ‚àí 6 ùë• \n",
    "\n",
    "Second derivative (Hessian): ùëì ‚Ä≤ ‚Ä≤ ( ùë• ) = 6 ùë• ‚àí 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef5f1e",
   "metadata": {},
   "source": [
    "#### Step 3: Newton's Method Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e73886",
   "metadata": {},
   "source": [
    "Choose an appropriate initial point ùë• 0  and iterate using Newton's update formula: ùë• (ùëò + 1) = ùë• ùëò ‚àí ùëì ‚Ä≤ ( ùë• ùëò ) ùëì ‚Ä≤ ‚Ä≤ ( ùë• ùëò )  Repeat this process until the value of ùë• x converges (i.e., the difference between ùë•( ùëò + 1) and ùë• ùëò  is very small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3b945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum value of x found is: -1.1776091950196715e-10\n",
      "The function value at this point is: 2.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3 - 3*x**2 + 2\n",
    "\n",
    "def df(x):\n",
    "    return 3*x**2 - 6*x\n",
    "\n",
    "def ddf(x):\n",
    "    return 6*x - 6\n",
    "\n",
    "def newton_method(x0, epsilon=1e-6, max_iter=1000):\n",
    "    x = x0\n",
    "    for _ in range(max_iter):\n",
    "        x_new = x - df(x) / ddf(x)\n",
    "        if abs(x_new - x) < epsilon:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "# Initial guess\n",
    "x0 = 0.1\n",
    "minimum = newton_method(x0)\n",
    "print(\"The minimum value of x found is:\", minimum)\n",
    "print(\"The function value at this point is:\", f(minimum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061a18e",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "Convergence: The convergence of Newton's Method depends on the choice of the initial point and the nature of the function. In some cases, if the Hessian matrix is not positive definite during iterations, or if the initial point is poorly chosen, the method may not converge.\n",
    "\n",
    "Adjustments: In practical applications, it may be necessary to make adjustments to Newton's Method, such as introducing step size control (line search) or using quasi-Newton methods to avoid direct computation of the Hessian matrix.\n",
    "\n",
    "By following this approach, we can effectively use Newton's Method to solve practical minimization problems. The provided code and steps offer a basic framework that can be adjusted and optimized according to specific problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52296626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
